{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c709f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'\n",
    "messages = [ {\"role\": \"system\", \"content\": \"You are a intelligent assistant.\"} ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf7b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_chatgpt(_messages, prompt, _model=\"gpt-3.5-turbo\"):\n",
    "\n",
    "# with only the current clip as information\n",
    "    chat = openai.ChatCompletion.create(\n",
    "        model=_model, messages= [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "#   with all the context in the same video clip $TOO LARGE TOKEN SIZE\n",
    "#   _messages.append({\"role\": \"user\", \"content\": prompt},)\n",
    "#   chat = openai.ChatCompletion.create(\n",
    "#       model=_model, messages=_messages\n",
    "#   )\n",
    "\n",
    "    reply = chat.choices[0].message.content\n",
    "#     _messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "508d03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQA(data_file):\n",
    "    QA = {}\n",
    "    \n",
    "    f = open(data_file)\n",
    "    \n",
    "    data = json.load(f)\n",
    "    for video_id in data:\n",
    "        messages = [ {\"role\": \"system\", \"content\": \n",
    "              \"You are a intelligent assistant.\"} ]\n",
    "        QA[video_id] = {}\n",
    "        QA[video_id]['annotation_qa'] = []\n",
    "        QA[video_id]['summary_qa'] = []\n",
    "        video = data[video_id]\n",
    "        \n",
    "        i = 0\n",
    "        for item in video['narrations']:\n",
    "            if i == 20:\n",
    "                break;\n",
    "            prompt = get_prompt(' start_time: ' + str(item['time']) \n",
    "                                + ' end_time: ' + str(item['_end_time']) \n",
    "                                + ' description: ' + str(item['text']))\n",
    "            QA[video_id]['annotation_qa'].append(chat_with_chatgpt(messages, prompt))\n",
    "            i += 1\n",
    "        \n",
    "        i = 0\n",
    "        for item in video['summaries']:\n",
    "            if i == 20:\n",
    "                break;\n",
    "            prompt = get_prompt(' start_time: ' + str(item['start_time']) \n",
    "                                + ' end_time: ' + str(item['end_time']) \n",
    "                                + ' description: ' + str(item['text']))\n",
    "            QA[video_id]['summary_qa'].append(chat_with_chatgpt(messages, prompt))\n",
    "            i += 1\n",
    "\n",
    "    return QA\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1fefec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(data_location):\n",
    "    return \"\"\"\n",
    "        Your task is to create a comprehensive set of question-and-answer(QA) pairs for each time slot of an egocentric video. Your data input would be start time, end time, and the description of event during the time period. The questions and answers should be designed to facilitate machine learning and test understanding for computers, covering key concepts and important information presented in the video. Create a final QA pair to the given questions using the provided document excerpts(in no particular order) as references. If you are unable to answer the question you are given or unable to generate a question, simply state that you do not know. Do not attempt to fabricate an answer and leave the Question and Answer section empty.\n",
    "        ---------\n",
    "        Here is an example of the format of the data input and its QA pairs:\\n\n",
    "        Description: '#C C drops the vegetable on the chair with other vegetables'\n",
    "        =========\n",
    "        Q: What does C do?\n",
    "        A: C drops the vegetable on the chair with other vegetables\n",
    "\n",
    "        Q: What does C drop?\n",
    "        A: the vegetable\n",
    "\n",
    "        Q: Where does C drop the vegetable?\n",
    "        A: on the chair\n",
    "        =========\n",
    "        Please format your results as follows: 'Q: <question>  \\n  A: <answer>.' + start_time + end_time\n",
    "\n",
    "        Now complete your task given the start time, end time, and description of the video as the following: %s0\n",
    "\n",
    "        \"\"\" % data_location\n",
    "\n",
    "    # \"\"\"\n",
    "    # You are given the description of a video clip, and the data has a start time, end time, and description of the clip.\n",
    "    # You must generate a set of question answer pairs for this video clip.\n",
    "    # Ask the question \"When does (your detailed description of the object) appear?\" for every physical object (including things like keys, spoon, and phone but does not including non-physical object such as a conversation or a movement) that appears in the description.\n",
    "    # And the answer must be in the format: start_time: , end_time: .\n",
    "    # If there is no physical object in the description, answer \"no objects\"\n",
    "    # Here is the format:\\n Q:\\n A:\\nâ€™.\n",
    "    \n",
    "    # Now complete your task given the start time, end time, and context of the video as the following: %s0\n",
    "    # \"\"\" % data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eeb27d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "<empty message>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m QA \u001b[39m=\u001b[39m generateQA(\u001b[39m'\u001b[39;49m\u001b[39mout.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb Cell 5\u001b[0m in \u001b[0;36mgenerateQA\u001b[0;34m(data_file)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m;\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     prompt \u001b[39m=\u001b[39m get_prompt(\u001b[39m'\u001b[39m\u001b[39m start_time: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(item[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m end_time: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(item[\u001b[39m'\u001b[39m\u001b[39m_end_time\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m description: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(item[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     QA[video_id][\u001b[39m'\u001b[39m\u001b[39mannotation_qa\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(chat_with_chatgpt(messages, prompt))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;32m/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb Cell 5\u001b[0m in \u001b[0;36mchat_with_chatgpt\u001b[0;34m(_messages, prompt, _model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchat_with_chatgpt\u001b[39m(_messages, prompt, _model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# with only the current clip as information\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     chat \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         model\u001b[39m=\u001b[39;49m_model, messages\u001b[39m=\u001b[39;49m [{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt}]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#   with all the context in the same video clip $TOO LARGE TOKEN SIZE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#   _messages.append({\"role\": \"user\", \"content\": prompt},)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#   chat = openai.ChatCompletion.create(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#       model=_model, messages=_messages\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#   )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jackgengqc/code/appteam/academy/Visual-Query/QA_automation/QA_generation.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     reply \u001b[39m=\u001b[39m chat\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: <empty message>"
     ]
    }
   ],
   "source": [
    "QA = generateQA('out.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14775cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ego4DQA.json', 'w') as wf:\n",
    "    json.dump(QA, wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
